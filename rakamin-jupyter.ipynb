{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\ntry:\n    import pycaret\n    import pyspark\nexcept:\n    os.system(\"pip install -q pycaret\")\n    os.system('pip install -q pyspark')\n    os.system('pip install -q graphviz')\n    os.system(\"pip install -q pydot\")\n    pass","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:05:57.745365Z","iopub.execute_input":"2023-06-13T10:05:57.745924Z","iopub.status.idle":"2023-06-13T10:05:57.753193Z","shell.execute_reply.started":"2023-06-13T10:05:57.745876Z","shell.execute_reply":"2023-06-13T10:05:57.752023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-13T10:05:57.755105Z","iopub.execute_input":"2023-06-13T10:05:57.755926Z","iopub.status.idle":"2023-06-13T10:05:57.776142Z","shell.execute_reply.started":"2023-06-13T10:05:57.755891Z","shell.execute_reply":"2023-06-13T10:05:57.774515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add Function","metadata":{}},{"cell_type":"code","source":"import re\n\ndef tree_to_dot(tree_str, feature_names=None):\n    dot_str = 'digraph Tree {\\nnode [shape=box] ;\\n'\n    lines = tree_str.split('\\n')\n    stack = []\n    node_id = 0\n    for line in lines:\n        if line.strip() == '':\n            continue\n        depth = len(re.findall('\\s\\s', line))\n        line = line.strip()\n        if 'If' in line:\n            if ' <= ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' <= ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} <= {value}\"] ;\\n'\n            elif ' > ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' > ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} > {value}\"] ;\\n'\n            elif ' in ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' in ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} in {value}\"] ;\\n'\n            elif ' not in ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' not in ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} not in {value}\"] ;\\n'\n            if len(stack) > 0:\n                dot_str += f'{stack[-1]} -> {node_id} [labeldistance=2.5, labelangle=45, headlabel=\"True\"];\\n'\n            stack.append(node_id)\n            node_id += 1\n        elif 'Predict' in line:\n            value = re.findall(r': ([^ ]+)', line)[0]\n            dot_str += f'{node_id} [label=\"Predict: {value}\", fillcolor=\"#e5813900\"] ;\\n'\n            dot_str += f'{stack[-1]} -> {node_id} ;\\n'\n            node_id += 1\n        elif 'Else' in line:\n            stack = stack[:depth]\n            if ' > ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' > ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} > {value}\"] ;\\n'\n            elif ' <= ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' <= ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} <= {value}\"] ;\\n'\n            elif ' not in ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' not in ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} not in {value}\"] ;\\n'\n            elif ' in ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' in ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} in {value}\"] ;\\n'\n            dot_str += f'{stack[-1]} -> {node_id} [labeldistance=2.5, labelangle=-45, headlabel=\"False\"];\\n'\n            stack.append(node_id)\n            node_id += 1\n    dot_str += '}'\n    return dot_str","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:05:57.778171Z","iopub.execute_input":"2023-06-13T10:05:57.779008Z","iopub.status.idle":"2023-06-13T10:05:57.799860Z","shell.execute_reply.started":"2023-06-13T10:05:57.778770Z","shell.execute_reply":"2023-06-13T10:05:57.798568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.classification import DecisionTreeClassificationModel\n# from pyspark.ml.tree import Node\n\ndef extract_rules(node, feature_names, rule_string=\"\"):\n    if node.numChildren() == 0:\n        return rule_string + \"Predict: \" + str(node.prediction())\n\n    feature = node.split().featureIndex()\n    threshold = node.split().threshold()\n    feature_name = feature_names[feature]\n    condition = \" <= \" if node.split().leftSide() else \" > \"\n    rule_string += \"If (\" + feature_name + condition + str(threshold) + \")\\n\"\n\n    left_child_rule = extract_rules(node.leftChild(), feature_names, rule_string + \"  \")\n    right_child_rule = extract_rules(node.rightChild(), feature_names, rule_string + \"  \")\n    return left_child_rule + \"\\n\" + right_child_rule\n\ndef decision_tree_to_dot_data(dt_model, feature_names):\n    tree_model = dt_model.stages[-1]\n    root_node = tree_model._call_java(\"rootNode\")\n\n    dot_data = \"digraph Tree {\\n\"\n    dot_data += \"node [shape=box]\\n\"\n\n    rules = extract_rules(root_node, feature_names)\n    dot_data += rules\n\n    dot_data += \"}\"\n    return dot_data","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:05:57.801602Z","iopub.execute_input":"2023-06-13T10:05:57.802018Z","iopub.status.idle":"2023-06-13T10:05:57.821518Z","shell.execute_reply.started":"2023-06-13T10:05:57.801977Z","shell.execute_reply":"2023-06-13T10:05:57.820148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport scipy.stats as stats\nimport numpy as np\n\ndef cramers_v(df):\n    result = pd.DataFrame(index=df.columns, columns=df.columns)\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                contingency_table = pd.crosstab(df[col1], df[col2])\n                X2 = stats.chi2_contingency(contingency_table, correction=False)[0]\n                n = np.sum(contingency_table.to_numpy())\n                minDim = min(contingency_table.shape)-1\n                V = np.sqrt((X2/n) / minDim)\n                result.loc[col1, col2] = V\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:05:57.824368Z","iopub.execute_input":"2023-06-13T10:05:57.825286Z","iopub.status.idle":"2023-06-13T10:05:57.839658Z","shell.execute_reply.started":"2023-06-13T10:05:57.825244Z","shell.execute_reply":"2023-06-13T10:05:57.838470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calculate_vif_numeric(df):\n    # calculate VIF for each predictor variable\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n\n    return vif_data\n\ndef calculate_vif_categorical(df):\n    # create dummy variables for categorical variables\n    df_dummies = pd.get_dummies(df, drop_first=True)\n\n    # calculate VIF for each predictor variable\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = df_dummies.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df_dummies.values, i) for i in range(len(df_dummies.columns))]\n\n    return vif_data","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:05:57.866167Z","iopub.execute_input":"2023-06-13T10:05:57.866613Z","iopub.status.idle":"2023-06-13T10:05:57.876697Z","shell.execute_reply.started":"2023-06-13T10:05:57.866574Z","shell.execute_reply":"2023-06-13T10:05:57.875483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import col\n\ndef selectByTypePyspark(colType, df):\n    cols = [field.name for field in df.schema.fields if field.dataType == colType]\n    return df.select(*cols)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:05:57.879726Z","iopub.execute_input":"2023-06-13T10:05:57.880506Z","iopub.status.idle":"2023-06-13T10:05:57.894661Z","shell.execute_reply.started":"2023-06-13T10:05:57.880469Z","shell.execute_reply":"2023-06-13T10:05:57.893769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deselectByTypePyspark(colType, df):\n    cols = [field.name for field in df.schema.fields if field.dataType != colType]\n    return df.select(*cols)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:05:57.896348Z","iopub.execute_input":"2023-06-13T10:05:57.897338Z","iopub.status.idle":"2023-06-13T10:05:57.910010Z","shell.execute_reply.started":"2023-06-13T10:05:57.897304Z","shell.execute_reply":"2023-06-13T10:05:57.908865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef parse_tree_string(tree_string):\n    # Split the tree string into lines\n    lines = tree_string.split('\\n')\n    \n    # Initialize variables\n    nodes = []\n    stack = []\n    \n    # Iterate over the lines\n    for line in lines:\n        # Skip the first line\n        if line.startswith('DecisionTreeClassificationModel'):\n            continue\n        \n        # Find the depth of the current node\n        depth = line.find('If')\n        \n        # Check if the line represents a decision node\n        if depth >= 0:\n            # Find the feature and threshold of the current node\n            feature, threshold = re.findall(r'feature (\\d+) <= ([\\d.]+)', line)[0]\n            \n            # Create a new node\n            node = {'feature': int(feature), 'threshold': float(threshold), 'left': None, 'right': None}\n        else:\n            # Find the prediction of the current node\n            prediction = re.findall(r'Predict: ([\\d.]+)', line)[0]\n            \n            # Create a new node\n            node = {'prediction': float(prediction)}\n            \n            # Set the depth of the current node\n            depth = len(stack) - 1\n        \n        # Add the node to the list of nodes\n        nodes.append(node)\n        \n        # Update the stack\n        while len(stack) > depth:\n            stack.pop()\n        \n        # Update the parent node\n        if len(stack) > 0:\n            parent_node = nodes[stack[-1]]\n            if parent_node['left'] is None:\n                parent_node['left'] = len(nodes) - 1\n            else:\n                parent_node['right'] = len(nodes) - 1\n        \n        # Add the current node to the stack\n        stack.append(len(nodes) - 1)\n    \n    # Return the root node\n    return nodes[0]\n","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:05:57.911445Z","iopub.execute_input":"2023-06-13T10:05:57.912491Z","iopub.status.idle":"2023-06-13T10:05:57.927083Z","shell.execute_reply.started":"2023-06-13T10:05:57.912453Z","shell.execute_reply":"2023-06-13T10:05:57.925691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from graphviz import Digraph\n\ndef plot_tree(node, graph=None, parent=None, edge_label=None):\n    # Create a new graph\n    if graph is None:\n        graph = Digraph()\n    \n    # Get the node ID\n    node_id = str(id(node))\n    \n    # Add the node to the graph\n    if 'prediction' in node:\n        graph.node(node_id, label=str(node['prediction']))\n    else:\n        graph.node(node_id, label=f'X[{node[\"feature\"]}] <= {node[\"threshold\"]}')\n    \n    # Add the edge to the graph\n    if parent is not None:\n        graph.edge(parent, node_id, label=edge_label)\n    \n    # Recursively plot the child nodes\n    if 'left' in node:\n        plot_tree(node['left'], graph=graph, parent=node_id, edge_label='True')\n    if 'right' in node:\n        plot_tree(node['right'], graph=graph, parent=node_id, edge_label='False')\n    \n    # Return the graph\n    return graph","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:05:57.931375Z","iopub.execute_input":"2023-06-13T10:05:57.932153Z","iopub.status.idle":"2023-06-13T10:05:57.947711Z","shell.execute_reply.started":"2023-06-13T10:05:57.932105Z","shell.execute_reply":"2023-06-13T10:05:57.946621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Input Data","metadata":{}},{"cell_type":"code","source":"loan_df = pd.read_csv('/kaggle/input/rakamin-idx/loan_data_2007_2014.csv')\nloan_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:05:57.949482Z","iopub.execute_input":"2023-06-13T10:05:57.950598Z","iopub.status.idle":"2023-06-13T10:06:07.025620Z","shell.execute_reply.started":"2023-06-13T10:05:57.950558Z","shell.execute_reply":"2023-06-13T10:06:07.024479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning","metadata":{}},{"cell_type":"code","source":"reject_stats = ['url',\n 'zip_code',\n 'title',\n 'mths_since_last_record',\n 'mths_since_last_delinq',\n 'issue_d',\n 'inq_last_6mths',\n 'emp_title',\n 'emp_length',\n 'earliest_cr_line',\n 'desc',\n 'addr_state'\n]","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:07.026903Z","iopub.execute_input":"2023-06-13T10:06:07.027292Z","iopub.status.idle":"2023-06-13T10:06:07.033519Z","shell.execute_reply.started":"2023-06-13T10:06:07.027251Z","shell.execute_reply":"2023-06-13T10:06:07.032320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df = loan_df.drop(reject_stats,axis=1)\nloan_df","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:07.035188Z","iopub.execute_input":"2023-06-13T10:06:07.035588Z","iopub.status.idle":"2023-06-13T10:06:07.272437Z","shell.execute_reply.started":"2023-06-13T10:06:07.035552Z","shell.execute_reply":"2023-06-13T10:06:07.271148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df.term.unique()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:07.274053Z","iopub.execute_input":"2023-06-13T10:06:07.274833Z","iopub.status.idle":"2023-06-13T10:06:07.323891Z","shell.execute_reply.started":"2023-06-13T10:06:07.274796Z","shell.execute_reply":"2023-06-13T10:06:07.322652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df.term.replace({' 36 months':'36 months',\n                      ' 60 months':'60 months'},\n                     inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:07.325806Z","iopub.execute_input":"2023-06-13T10:06:07.326227Z","iopub.status.idle":"2023-06-13T10:06:07.463056Z","shell.execute_reply.started":"2023-06-13T10:06:07.326192Z","shell.execute_reply":"2023-06-13T10:06:07.461488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del_col = loan_df.isna().sum()[loan_df.isna().sum()==(loan_df.shape[0])].index\ndel_col","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:07.464478Z","iopub.execute_input":"2023-06-13T10:06:07.465311Z","iopub.status.idle":"2023-06-13T10:06:11.323499Z","shell.execute_reply.started":"2023-06-13T10:06:07.465271Z","shell.execute_reply":"2023-06-13T10:06:11.322323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df = loan_df.drop(del_col,axis=1)\nloan_df","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:11.325079Z","iopub.execute_input":"2023-06-13T10:06:11.325426Z","iopub.status.idle":"2023-06-13T10:06:11.621880Z","shell.execute_reply.started":"2023-06-13T10:06:11.325398Z","shell.execute_reply":"2023-06-13T10:06:11.620786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df = loan_df.drop(\"Unnamed: 0\",axis=1)\nloan_df","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:11.623393Z","iopub.execute_input":"2023-06-13T10:06:11.623741Z","iopub.status.idle":"2023-06-13T10:06:11.957637Z","shell.execute_reply.started":"2023-06-13T10:06:11.623710Z","shell.execute_reply":"2023-06-13T10:06:11.956194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df = loan_df.drop(['id','member_id'],axis=1)\nloan_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:11.964836Z","iopub.execute_input":"2023-06-13T10:06:11.965270Z","iopub.status.idle":"2023-06-13T10:06:12.136713Z","shell.execute_reply.started":"2023-06-13T10:06:11.965236Z","shell.execute_reply":"2023-06-13T10:06:12.135488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"miss_col = loan_df.isna().sum()[loan_df.isna().sum()!=0]/loan_df.shape[0]*100\nmiss_col","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:12.139031Z","iopub.execute_input":"2023-06-13T10:06:12.139520Z","iopub.status.idle":"2023-06-13T10:06:15.922431Z","shell.execute_reply.started":"2023-06-13T10:06:12.139475Z","shell.execute_reply":"2023-06-13T10:06:15.921107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"miss_col = miss_col[miss_col>20]\nmiss_col","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:15.924199Z","iopub.execute_input":"2023-06-13T10:06:15.924875Z","iopub.status.idle":"2023-06-13T10:06:15.935171Z","shell.execute_reply.started":"2023-06-13T10:06:15.924832Z","shell.execute_reply":"2023-06-13T10:06:15.933999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"miss_col = miss_col.index.tolist()\nmiss_col","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:15.936910Z","iopub.execute_input":"2023-06-13T10:06:15.937541Z","iopub.status.idle":"2023-06-13T10:06:15.949389Z","shell.execute_reply.started":"2023-06-13T10:06:15.937506Z","shell.execute_reply":"2023-06-13T10:06:15.948086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df = loan_df.drop(miss_col,axis=1)\nloan_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:15.951099Z","iopub.execute_input":"2023-06-13T10:06:15.951451Z","iopub.status.idle":"2023-06-13T10:06:16.082680Z","shell.execute_reply.started":"2023-06-13T10:06:15.951422Z","shell.execute_reply":"2023-06-13T10:06:16.081565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for colname in loan_df.columns:\n    banyak = len(loan_df[colname].unique())\n    if(banyak==1):\n        print(colname,banyak)\nloan_df = loan_df.drop(['policy_code','application_type'],axis=1)\nloan_df","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:16.084508Z","iopub.execute_input":"2023-06-13T10:06:16.084835Z","iopub.status.idle":"2023-06-13T10:06:17.214496Z","shell.execute_reply.started":"2023-06-13T10:06:16.084806Z","shell.execute_reply":"2023-06-13T10:06:17.213382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df.select_dtypes(exclude='object').corr()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:17.216517Z","iopub.execute_input":"2023-06-13T10:06:17.217453Z","iopub.status.idle":"2023-06-13T10:06:18.393079Z","shell.execute_reply.started":"2023-06-13T10:06:17.217407Z","shell.execute_reply":"2023-06-13T10:06:18.392041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df.isna().sum()[loan_df.isna().sum()!=0]","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:18.394798Z","iopub.execute_input":"2023-06-13T10:06:18.396222Z","iopub.status.idle":"2023-06-13T10:06:21.701983Z","shell.execute_reply.started":"2023-06-13T10:06:18.396170Z","shell.execute_reply":"2023-06-13T10:06:21.701105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imputation","metadata":{}},{"cell_type":"code","source":"from statsmodels.imputation.mice import MICEData\n\nloan_df_num = loan_df.select_dtypes(exclude=['object','datetime64'])\nimp = MICEData(loan_df_num)\nimp.update_all(5)\nloan_df_num = imp.data\nloan_df[loan_df_num.columns.tolist()] = loan_df_num\nloan_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:06:21.703467Z","iopub.execute_input":"2023-06-13T10:06:21.704714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_vif = calculate_vif_numeric(loan_df_num)\nnumeric_vif = numeric_vif.sort_values('VIF',ascending=False)\nnumeric_vif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_vif = ['total_pymnt',\n            'out_prncp',\n            'funded_amnt',\n            'funded_amnt_inv',\n            'total_pymnt_inv',\n            'loan_amnt',\n            'installment',\n            'int_rate',\n            'open_acc'\n            ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_vif = calculate_vif_numeric(loan_df_num.drop(drop_vif,\n                                                     axis=1))\nnumeric_vif = numeric_vif.sort_values('VIF',ascending=False)\nnumeric_vif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df_num = loan_df_num[numeric_vif.feature.to_list()]\nloan_df_num","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df = loan_df.drop(drop_vif,axis=1)\nloan_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df.last_pymnt_d = pd.to_datetime(loan_df.last_pymnt_d,format=\"%b-%y\")\nloan_df.last_credit_pull_d = pd.to_datetime(loan_df.last_credit_pull_d,format=\"%b-%y\")\nloan_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cramers_v(loan_df.select_dtypes(include='object'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"objek_vif = calculate_vif_categorical(loan_df.select_dtypes(include='object').drop(['sub_grade'],axis=1))\nobjek_vif = objek_vif.sort_values('VIF',ascending=False)\nobjek_vif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df = loan_df.drop(['sub_grade'],axis=1)\nloan_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot(data=loan_df,\n              y='loan_status',\n              order=loan_df['loan_status'].value_counts().index\n             )\n\n# Add annotations to display the count of each category\nfor p in plt.gca().patches:\n    plt.gca().text(p.get_width(), \n                   p.get_y() + p.get_height() / 2.,\n                   f'{int(p.get_width())}',\n                   va='center',\n                  )\n# Show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dari Data ini membuktikan bahwa :\n* Disini ada lebih banyak status utang, current ,fully paid, dan chargeoff\n* Ada terjadinya ketidakseimbangan antar kelasnya","metadata":{}},{"cell_type":"code","source":"# sns.pairplot(loan_df.select_dtypes(include=['float64']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Example MLspark","metadata":{}},{"cell_type":"code","source":"from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nfrom pyspark.sql import SparkSession\n\n\n# Create an instance of SparkSession\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\n# Create a larger dummy dataset with 20 rows, multiple categorical and numeric columns, and a multiclass label column\ndata = spark.createDataFrame([\n    (\"A\", \"X\", \"M\", \"S\", \"U\", \"K\", 1.0, 2.0, 3.0, 0),\n    (\"B\", \"Y\", \"N\", \"T\", \"V\", \"L\", 2.0, 3.0, 4.0, 1),\n    (\"C\", \"Z\", \"O\", \"U\", \"W\", \"M\", 3.0, 4.0, 5.0, 2),\n    (\"A\", \"X\", \"P\", \"V\", \"X\", \"N\", 4.0, 5.0, 6.0, 2),\n    (\"B\", \"Y\", \"Q\", \"W\", \"Y\", \"O\", 5.0, 6.0, 7.0, 1),\n    (\"C\", \"Z\", \"R\", \"X\", \"Z\", \"P\", 6.0, 7.0, 8.0, 2),\n    (\"A\", \"X\", \"S\", \"Y\", \"A\", \"Q\", 7.0, 8.0, 9.0, 1),\n    (\"B\" ,\"Y\" ,\"T\" ,\"Z\" ,\"B\" ,\"R\" ,8.0 ,9.0 ,10.0 ,2),\n    (\"C\" ,\"Z\" ,\"U\" ,\"A\" ,\"C\" ,\"S\" ,9.0 ,10.0 ,11.0 ,1),\n    (\"A\" ,\"X\" ,\"V\" ,\"B\" ,\"D\" ,\"T\" ,10.0 ,11.0 ,12.0 ,2),\n    (\"B\" ,\"Y\" ,\"W\" ,\"C\" ,\"E\" ,\"U\" ,11.0 ,12.0 ,13.0 ,1),\n    (\"C\" ,\"Z\" ,\"X\" ,\"D\" ,\"F\" ,\"V\" ,12.0 ,13.0 ,14.0 ,2),\n    (\"A\" ,\"X\" ,\"Y\" ,\"E\" ,\"G\" ,\"W\" ,13.0 ,14.0 ,15.0 ,1),\n    (\"B\",\"Y\",\"Z\",\"F\",\"H\",\"X\",14.0,15.0,16.0,2),\n    (\"C\",\"Z\",\"A\",\"G\",\"I\",\"Y\",15.0,16.0,17.0,1),\n    (\"A\",\"X\",\"B\",\"H\",\"J\",\"Z\",16.0,17.0,18.0,2),\n    (\"B\",\"Y\",\"C\",\"I\",\"K\",\"A\",17.0,18.0,19.0,1),\n    (\"C\",\"Z\",\"D\",\"J\",\"L\",\"B\",18.0,19.0,20.0,2),\n    (\"A\",\"X\",\"E\",\"K\",\"M\",\"C,\",19.0,20.0,21.0,1),\n    (\"B\",'Y','F','L','N','D',20.0,21.0,22.0,2)\n], [\"category1\",\n     'category2',\n     'category3',\n     'category4',\n     'category5',\n     'category6',\n     'numeric1',\n     'numeric2',\n     'numeric3',\n     'label'])\n\n\n\n# Define the categorical and numeric columns\ncategorical_cols = ['category1',\n                    'category2',\n                    'category3',\n                    'category4',\n                    'category5',\n                    'category6']\nnumeric_cols = ['numeric1','numeric2','numeric3']\n\n# Create instances of StringIndexer and OneHotEncoder for each categorical column\nindexers = [StringIndexer(inputCol=colu,outputCol=f\"{colu}Index\") for colu in categorical_cols]\nencoders = [OneHotEncoder(inputCol=f\"{colu}Index\", outputCol=f\"{colu}Vec\") for colu in categorical_cols]\n\n# Assemble the one-hot encoded and numeric columns into a single features column\nassembler = VectorAssembler(inputCols=[f\"{colu}Vec\"\n                                       for colu in categorical_cols] + numeric_cols,\n                            outputCol=\"features\")\n\n# Create an instance of DecisionTreeClassifier for multiclass classification\ndtc = DecisionTreeClassifier(labelCol=\"label\",\n                             featuresCol =\"features\")\n\n# Create a pipeline with the stages\npipeline = Pipeline(stages=indexers + encoders + [assembler] + [dtc])\n\n# Fit the pipeline to the data\nmodel = pipeline.fit(data)\n\n# Make predictions on the data\npredictions = model.transform(data)\n\nprint(predictions.show())\n# Show the predictions\npredictions.select(\"prediction\" ,\"label\").show()\n\n# Create an instance of MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\",\n                                              predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\n\n# Evaluate the model on the data\naccuracy = evaluator.evaluate(predictions)\n\n# Print the accuracy\nprint(f\"Accuracy: {accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning","metadata":{}},{"cell_type":"code","source":"loan_df = loan_df.fillna(loan_df.mode().iloc[0])\nloan_df.isna().sum()[loan_df.isna().sum()!=0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spark = SparkSession.builder.appName(\"PandasToPySpark\").getOrCreate()\nspark_df = spark.createDataFrame(loan_df)\nspark_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\n\ncat_col = ['term',\n           'grade',\n           'home_ownership',\n           'verification_status',\n           #'loan_status',\n           'pymnt_plan',\n           'purpose',\n           'initial_list_status']\n# One-hot encode the categorical variables\nindexers = [StringIndexer(inputCol=colu, outputCol=colu+\"_index\") for colu in cat_col]\nencoders = [OneHotEncoder(inputCol=colu+\"_index\", outputCol=colu+\"_vec\") for colu in cat_col]\nlabel_indexer = StringIndexer(inputCol=\"loan_status\", outputCol=\"loan_status_index\")\npipeline = Pipeline(stages=indexers + encoders + [label_indexer])\nspark_df = pipeline.fit(spark_df).transform(spark_df)\nspark_df.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spark_df = spark_df.drop('last_pymnt_d','last_credit_pull_d')\nspark_df.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.types import StringType\nspark_df_non_str = deselectByTypePyspark(StringType(),spark_df)\nspark_df_non_str","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cc = spark_df_non_str.columns\ncc.remove('loan_status_index')\ncc = [c for c in cc if (c.endswith('_index')==False)]\ncc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\ncoba_df = spark_df_non_str\n\nfrom pyspark.ml.feature import VectorAssembler\nassem = VectorAssembler(inputCols=cc,\n                        outputCol='feature_index',\n                       )\ncoba_df = assem.transform(coba_df)\n\nfrom pyspark.ml.feature import VectorAssembler\nassem_lab = VectorAssembler(inputCols=['loan_status_index'],\n                            outputCol='loan_status_vec',\n                       )\ncoba_df = assem_lab.transform(coba_df)\n\nprint(coba_df.select('feature_index').show(5))\n\n\ntrain_df,test_df = coba_df.randomSplit([0.8,0.2],seed=42)\ndt = DecisionTreeClassifier(featuresCol=\"feature_index\", \n                            labelCol=\"loan_status_index\",\n                            maxDepth=4,\n                            maxBins=40,\n                            minInstancesPerNode=2,\n                           )\nmodel = dt.fit(train_df)\npredictions = model.transform(test_df)\nprint(predictions.select('prediction','loan_status_index').show(5))\n\n\n\n# Create an instance of MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator(labelCol=\"loan_status_index\",\n                                              predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\n\n# Evaluate the model on the test dataset\naccuracy = evaluator.evaluate(predictions)\n# Print the accuracy\nprint(f\"Accuracy: {accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import dtreeviz\nimport graphviz\nimport pydot\nfrom pyspark.ml.classification import DecisionTreeClassificationModel\nfrom graphviz import Source\n\n\n# Extract the trained decision tree model\ntrained_model = model\n\n# Extract the tree structure and rules\ntree_string = trained_model.toDebugString\ndot_string = tree_to_dot(tree_string)\nsrc = Source(dot_string)\nsrc.format='png'\nsrc.render('decision-tree.gv', view=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as img\nfrom matplotlib.pyplot import figure\n\n\nim = img.imread('/kaggle/working/decision-tree.gv.png')\nfigure(figsize=(16, 8))\nplt.imshow(im)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dot_string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}